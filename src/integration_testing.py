#!/usr/bin/env python3
"""
Stock Market Prediction Engine
System Integration & Testing Framework
"""

import pandas as pd
import numpy as np
import asyncio
import json
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
import unittest
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Internal imports
from .config import Config
from .realtime_prediction import RealTimePredictionEngine
from .risk_management import RiskManagementFramework
from .validation_framework import ValidationFramework

class SystemIntegrationTester:
    def __init__(self):
        self.config = Config()
        self.test_results = {}
        self.performance_metrics = {}
        self.errors = []
        self.start_time = time.time()
        
    def log_test(self, test_name: str, status: str, details: str = "", execution_time: float = 0):
        self.test_results[test_name] = {
            'status': status,
            'details': details,
            'execution_time': execution_time,
            'timestamp': datetime.now().isoformat()
        }
        
        status_icon = "[PASS]" if status == "PASS" else "[FAIL]" if status == "FAIL" else "[WARN]"
        time_str = f"({execution_time:.2f}s)" if execution_time > 0 else ""
        print(f"{status_icon} {test_name} {time_str}")
        if details and status != "PASS":
            print(f"   {details}")
    
    def test_configuration_system(self) -> bool:
        test_start = time.time()
        
        try:
            config = Config()
            
            # Test path creation
            config.create_directories()
            
            # Verify essential paths exist
            required_paths = [
                config.DATA_PATH,
                config.RAW_DATA_PATH,
                config.PROCESSED_DATA_PATH,
                config.FEATURES_DATA_PATH,
                config.LOGS_PATH
            ]
            
            for path in required_paths:
                if not path.exists():
                    raise Exception(f"Required path missing: {path}")
            
            # Test configuration values
            if not hasattr(config, 'RANDOM_STATE') or config.RANDOM_STATE != 42:
                raise Exception("Configuration values incorrect")
            
            self.log_test("Configuration System", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("Configuration System", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Configuration: {e}")
            return False
    
    def test_data_pipeline(self) -> bool:
        """Test data loading and processing pipeline"""
        test_start = time.time()
        
        try:
            # Check for essential data files
            required_files = [
                self.config.FEATURES_DATA_PATH / "selected_features.csv",
                self.config.PROCESSED_DATA_PATH / "target_stocks.txt"
            ]
            
            missing_files = []
            for file_path in required_files:
                if not file_path.exists():
                    missing_files.append(str(file_path))
            
            if missing_files:
                # Create minimal test data
                self._create_test_data()
            
            # Test data loading
            features_path = self.config.FEATURES_DATA_PATH / "selected_features.csv"
            if features_path.exists():
                df = pd.read_csv(features_path)
                if df.empty or 'Date' not in df.columns:
                    raise Exception("Invalid feature data format")
            
            self.log_test("Data Pipeline", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("Data Pipeline", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Data Pipeline: {e}")
            return False
    
    def test_model_loading(self) -> bool:
        """Test model loading and initialization"""
        test_start = time.time()
        
        try:
            engine = RealTimePredictionEngine()
            success = engine.load_production_models()
            
            if not success:
                raise Exception("Model loading failed")
            
            if not engine.models:
                raise Exception("No models loaded")
            
            if not engine.feature_columns:
                raise Exception("Feature columns not loaded")
            
            # Test feature count
            expected_features = 73
            actual_features = len(engine.feature_columns)
            if actual_features != expected_features:
                self.log_test("Model Loading", "WARN", 
                            f"Feature count mismatch: {actual_features} vs {expected_features}",
                            time.time() - test_start)
            else:
                self.log_test("Model Loading", "PASS", "", time.time() - test_start)
            
            return True
            
        except Exception as e:
            self.log_test("Model Loading", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Model Loading: {e}")
            return False
    
    async def test_realtime_predictions(self) -> bool:
        """Test real-time prediction system"""
        test_start = time.time()
        
        try:
            engine = RealTimePredictionEngine()
            
            if not engine.load_production_models():
                raise Exception("Failed to load models for prediction test")
            
            # Run prediction cycle
            results = await engine.run_realtime_cycle()
            
            if not results:
                raise Exception("No prediction results returned")
            
            # Validate results structure
            required_keys = ['predictions', 'alerts', 'performance_metrics']
            for key in required_keys:
                if key not in results:
                    raise Exception(f"Missing key in results: {key}")
            
            # Check prediction quality
            predictions = results.get('predictions', {})
            if not predictions:
                self.log_test("Real-time Predictions", "WARN", "No predictions generated", time.time() - test_start)
            else:
                # Validate prediction format
                for symbol, pred_data in predictions.items():
                    if 'primary' not in pred_data:
                        raise Exception(f"Invalid prediction format for {symbol}")
                    
                    primary = pred_data['primary']
                    if 'prediction' not in primary:
                        raise Exception(f"Missing prediction value for {symbol}")
                
                self.log_test("Real-time Predictions", "PASS", 
                            f"Generated {len(predictions)} predictions", time.time() - test_start)
            
            return True
            
        except Exception as e:
            self.log_test("Real-time Predictions", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Real-time Predictions: {e}")
            return False
    
    def test_risk_management(self) -> bool:
        """Test risk management and portfolio optimization"""
        test_start = time.time()
        
        try:
            risk_framework = RiskManagementFramework()
            
            # Test data loading
            df = risk_framework.load_feature_data()
            if df.empty:
                raise Exception("Failed to load feature data for risk management")
            
            # Test data preparation
            X, y, feature_cols = risk_framework.prepare_portfolio_data(df)
            if X.empty or len(feature_cols) == 0:
                raise Exception("Data preparation failed")
            
            # Test VaR calculation
            sample_returns = np.random.normal(0, 0.02, 1000)
            var_metrics = risk_framework.calculate_value_at_risk(sample_returns)
            
            if 'var_historical' not in var_metrics:
                raise Exception("VaR calculation failed")
            
            self.log_test("Risk Management", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("Risk Management", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Risk Management: {e}")
            return False
    
    def test_performance_validation(self) -> bool:
        """Test performance validation system"""
        test_start = time.time()
        
        try:
            # Check for validation results
            validation_path = self.config.PROCESSED_DATA_PATH / "day10_validation_results.json"
            
            if validation_path.exists():
                with open(validation_path, 'r') as f:
                    validation_data = json.load(f)
                
                # Validate structure
                if isinstance(validation_data, dict):
                    if 'validation_results' in validation_data:
                        validation_results = validation_data['validation_results']
                    else:
                        validation_results = validation_data
                    
                    if not validation_results:
                        raise Exception("Empty validation results")
                    
                    self.log_test("Performance Validation", "PASS", 
                                f"Loaded {len(validation_results)} model validations", time.time() - test_start)
                else:
                    raise Exception("Invalid validation data format")
            else:
                self.log_test("Performance Validation", "WARN", "No validation results found", time.time() - test_start)
            
            return True
            
        except Exception as e:
            self.log_test("Performance Validation", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Performance Validation: {e}")
            return False
    
    def test_dashboard_components(self) -> bool:
        """Test dashboard component functionality"""
        test_start = time.time()
        
        try:
            # Test Streamlit dashboard import
            dashboard_path = self.config.PROJECT_ROOT / "src" / "streamlit_dashboard.py"
            if not dashboard_path.exists():
                raise Exception("Dashboard file not found")
            
            # Test component initialization (without full Streamlit)
            import sys
            sys.path.append(str(self.config.PROJECT_ROOT))
            
            # Basic import test
            spec = __import__('importlib.util').util.spec_from_file_location("dashboard", dashboard_path)
            if spec is None:
                raise Exception("Cannot load dashboard module")
            
            self.log_test("Dashboard Components", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("Dashboard Components", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Dashboard Components: {e}")
            return False
    
    def test_api_compatibility(self) -> bool:
        """Test API server compatibility"""
        test_start = time.time()
        
        try:
            # Test API server import
            api_path = self.config.PROJECT_ROOT / "src" / "api_server.py"
            if api_path.exists():
                # Basic import test without starting server
                with open(api_path, 'r') as f:
                    api_content = f.read()
                
                # Check for essential components
                required_components = ['FastAPI', 'HTTPException', 'app = FastAPI']
                for component in required_components:
                    if component not in api_content:
                        raise Exception(f"Missing API component: {component}")
                
                self.log_test("API Compatibility", "PASS", "", time.time() - test_start)
            else:
                self.log_test("API Compatibility", "WARN", "API server not found", time.time() - test_start)
            
            return True
            
        except Exception as e:
            self.log_test("API Compatibility", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"API Compatibility: {e}")
            return False
    
    def test_end_to_end_workflow(self) -> bool:
        """Test complete end-to-end workflow"""
        test_start = time.time()
        
        try:
            # Simulate complete workflow
            engine = RealTimePredictionEngine()
            
            # Load models
            if not engine.load_production_models():
                raise Exception("E2E: Model loading failed")
            
            # Get target stocks
            stocks = engine.get_target_stocks()
            if not stocks:
                raise Exception("E2E: No target stocks available")
            
            # Test feature engineering (with minimal data)
            test_data = self._create_minimal_stock_data(stocks[0])
            if test_data.empty:
                raise Exception("E2E: Test data creation failed")
            
            features_df = engine.engineer_realtime_features(test_data, stocks[0])
            if features_df.empty:
                raise Exception("E2E: Feature engineering failed")
            
            # Test prediction preparation
            feature_array = engine.prepare_prediction_features(features_df)
            if feature_array is None:
                raise Exception("E2E: Feature preparation failed")
            
            # Test prediction generation
            predictions = engine.generate_predictions(feature_array, stocks[0])
            if not predictions:
                raise Exception("E2E: Prediction generation failed")
            
            self.log_test("End-to-End Workflow", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("End-to-End Workflow", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"End-to-End: {e}")
            return False
    
    def test_error_handling(self) -> bool:
        """Test system error handling and resilience"""
        test_start = time.time()
        
        try:
            engine = RealTimePredictionEngine()
            
            # Test with invalid data
            empty_df = pd.DataFrame()
            result = engine.engineer_realtime_features(empty_df, "TEST")
            if not result.empty:
                self.log_test("Error Handling", "WARN", "Should handle empty data gracefully", time.time() - test_start)
            
            # Test with missing features
            test_features = np.array([[0] * 50])  # Wrong feature count
            try:
                predictions = engine.generate_predictions(test_features, "TEST")
                # Should handle gracefully
            except:
                pass  # Expected to fail gracefully
            
            self.log_test("Error Handling", "PASS", "", time.time() - test_start)
            return True
            
        except Exception as e:
            self.log_test("Error Handling", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Error Handling: {e}")
            return False
    
    def test_performance_benchmarks(self) -> bool:
        """Test system performance benchmarks"""
        test_start = time.time()
        
        try:
            engine = RealTimePredictionEngine()
            if not engine.load_production_models():
                raise Exception("Model loading required for performance test")
            
            # Benchmark prediction speed
            stocks = engine.get_target_stocks()[:3]  # Test with 3 stocks
            
            performance_times = []
            for _ in range(3):  # Run 3 iterations
                iteration_start = time.time()
                
                # Create minimal test data
                for stock in stocks:
                    test_data = self._create_minimal_stock_data(stock)
                    features_df = engine.engineer_realtime_features(test_data, stock)
                    if not features_df.empty:
                        feature_array = engine.prepare_prediction_features(features_df)
                        if feature_array is not None:
                            engine.generate_predictions(feature_array, stock)
                
                performance_times.append(time.time() - iteration_start)
            
            avg_time = np.mean(performance_times)
            
            # Performance criteria
            if avg_time > 10:  # 10 seconds per 3-stock cycle
                self.log_test("Performance Benchmarks", "WARN", 
                            f"Slow performance: {avg_time:.2f}s per cycle", time.time() - test_start)
            else:
                self.log_test("Performance Benchmarks", "PASS", 
                            f"Average cycle time: {avg_time:.2f}s", time.time() - test_start)
            
            self.performance_metrics['avg_prediction_cycle_time'] = avg_time
            return True
            
        except Exception as e:
            self.log_test("Performance Benchmarks", "FAIL", str(e), time.time() - test_start)
            self.errors.append(f"Performance: {e}")
            return False
    
    def _create_test_data(self):
        """Create minimal test data for testing"""
        # Create test feature data
        test_features = pd.DataFrame({
            'Date': pd.date_range('2024-01-01', periods=100),
            'Ticker': ['AAPL'] * 100,
            'Close': np.random.normal(150, 10, 100),
            'Volume': np.random.normal(1000000, 100000, 100)
        })
        
        # Add dummy features to reach expected count
        for i in range(70):  # Add features to reach ~73 total
            test_features[f'feature_{i}'] = np.random.normal(0, 1, 100)
        
        features_path = self.config.FEATURES_DATA_PATH / "selected_features.csv"
        features_path.parent.mkdir(parents=True, exist_ok=True)
        test_features.to_csv(features_path, index=False)
        
        # Create test target stocks
        stocks_path = self.config.PROCESSED_DATA_PATH / "target_stocks.txt"
        stocks_path.parent.mkdir(parents=True, exist_ok=True)
        with open(stocks_path, 'w') as f:
            for stock in ['AAPL', 'AMZN', 'NVDA', 'MSFT', 'AMD']:
                f.write(f"{stock}\n")
    
    def _create_minimal_stock_data(self, symbol: str) -> pd.DataFrame:
        """Create minimal stock data for testing"""
        dates = pd.date_range(end=datetime.now(), periods=250, freq='D')
        
        # Generate realistic stock data
        base_price = 100
        returns = np.random.normal(0.001, 0.02, len(dates))
        prices = base_price * np.cumprod(1 + returns)
        
        data = pd.DataFrame({
            'Date': dates,
            'Ticker': symbol,
            'Open': prices * (1 + np.random.normal(0, 0.005, len(dates))),
            'High': prices * (1 + np.abs(np.random.normal(0.01, 0.01, len(dates)))),
            'Low': prices * (1 - np.abs(np.random.normal(0.01, 0.01, len(dates)))),
            'Close': prices,
            'Volume': np.random.normal(1000000, 200000, len(dates)),
            'Stock_Splits': 0
        })
        
        return data
    
    def generate_test_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'PASS')
        failed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'FAIL')
        warned_tests = sum(1 for result in self.test_results.values() if result['status'] == 'WARN')
        
        total_time = time.time() - self.start_time
        
        report = {
            'test_summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'warnings': warned_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                'total_execution_time': total_time
            },
            'test_results': self.test_results,
            'performance_metrics': self.performance_metrics,
            'errors': self.errors,
            'system_status': 'HEALTHY' if failed_tests == 0 else 'DEGRADED' if failed_tests < 3 else 'CRITICAL',
            'recommendations': self._generate_recommendations(),
            'timestamp': datetime.now().isoformat()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate system improvement recommendations"""
        recommendations = []
        
        failed_tests = [name for name, result in self.test_results.items() if result['status'] == 'FAIL']
        
        if 'Configuration System' in failed_tests:
            recommendations.append("Fix configuration system - ensure all paths are properly set")
        
        if 'Model Loading' in failed_tests:
            recommendations.append("Verify model files exist and are properly saved from previous days")
        
        if 'Data Pipeline' in failed_tests:
            recommendations.append("Check data files from feature engineering and validation")
        
        if 'Real-time Predictions' in failed_tests:
            recommendations.append("Debug prediction engine - check feature engineering and model compatibility")
        
        if self.performance_metrics.get('avg_prediction_cycle_time', 0) > 5:
            recommendations.append("Optimize prediction performance - consider feature reduction or model optimization")
        
        if len(self.errors) > 3:
            recommendations.append("Implement better error handling and logging throughout the system")
        
        if not recommendations:
            recommendations.append("System is performing well")
        
        return recommendations
    
    async def run_comprehensive_tests(self) -> Dict[str, Any]:
        """Run all integration tests"""
        print("System Integration Testing Started")
        print("=" * 50)
        
        # Core system tests
        self.test_configuration_system()
        self.test_data_pipeline()
        self.test_model_loading()
        
        # Functional tests
        await self.test_realtime_predictions()
        self.test_risk_management()
        self.test_performance_validation()
        
        # Integration tests
        self.test_dashboard_components()
        self.test_api_compatibility()
        self.test_end_to_end_workflow()
        
        # Quality tests
        self.test_error_handling()
        self.test_performance_benchmarks()
        
        # Generate report
        report = self.generate_test_report()
        
        print("\n" + "=" * 50)
        print("Integration Testing Complete")
        
        return report

class SystemOptimizer:
    """System optimization and performance tuning"""
    
    def __init__(self):
        self.config = Config()
        self.optimization_results = {}
    
    def optimize_prediction_pipeline(self) -> Dict[str, Any]:
        """Optimize prediction pipeline performance"""
        results = {
            'optimizations_applied': [],
            'performance_improvements': {},
            'recommendations': []
        }
        
        try:
            # Feature optimization
            features_path = self.config.FEATURES_DATA_PATH / "selected_features_list.txt"
            if features_path.exists():
                with open(features_path, 'r') as f:
                    features = [line.strip() for line in f.readlines()]
                
                # Optimize feature list (remove redundant features)
                if len(features) > 73:
                    optimized_features = features[:73]  # Keep top 73
                    
                    # Save optimized feature list
                    with open(self.config.FEATURES_DATA_PATH / "optimized_features.txt", 'w') as f:
                        for feature in optimized_features:
                            f.write(f"{feature}\n")
                    
                    results['optimizations_applied'].append("Feature list optimized to 73 features")
                    results['performance_improvements']['feature_reduction'] = len(features) - 73
            
            # Model caching optimization
            models_dir = self.config.PROJECT_ROOT / "models"
            if models_dir.exists():
                model_files = list(models_dir.rglob("*.joblib"))
                if len(model_files) > 10:
                    results['recommendations'].append("Consider model pruning to reduce memory usage")
            
            # Data caching optimization
            cache_dir = self.config.PROCESSED_DATA_PATH / "cache"
            cache_dir.mkdir(exist_ok=True)
            results['optimizations_applied'].append("Cache directory created for performance")
            
            return results
            
        except Exception as e:
            results['error'] = str(e)
            return results
    
    def cleanup_system_files(self) -> Dict[str, Any]:
        """Clean up temporary and redundant system files"""
        results = {
            'files_cleaned': 0,
            'space_freed_mb': 0,
            'cleaned_directories': []
        }
        
        try:
            # Clean temporary files
            temp_patterns = ['*.tmp', '*.temp', '*~', '.DS_Store']
            
            for pattern in temp_patterns:
                temp_files = list(self.config.PROJECT_ROOT.rglob(pattern))
                for temp_file in temp_files:
                    if temp_file.exists():
                        size_mb = temp_file.stat().st_size / (1024 * 1024)
                        temp_file.unlink()
                        results['files_cleaned'] += 1
                        results['space_freed_mb'] += size_mb
            
            # Clean empty directories
            for dir_path in self.config.PROJECT_ROOT.rglob('*'):
                if dir_path.is_dir() and not any(dir_path.iterdir()):
                    try:
                        dir_path.rmdir()
                        results['cleaned_directories'].append(str(dir_path))
                    except:
                        pass
            
            return results
            
        except Exception as e:
            results['error'] = str(e)
            return results